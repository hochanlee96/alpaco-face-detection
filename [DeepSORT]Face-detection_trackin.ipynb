{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87444960-4cf4-43c4-9bd1-4ae72cf3bd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python exe: /opt/miniconda3/envs/face-detection/bin/python\n",
      "Version: 3.10.18 (main, Jun  5 2025, 08:37:47) [Clang 14.0.6 ]\n",
      "Site-packages: ['/opt/miniconda3/envs/face-detection/lib/python3.10/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys, site\n",
    "print(\"Python exe:\", sys.executable)\n",
    "print(\"Version:\", sys.version)\n",
    "print(\"Site-packages:\", site.getsitepackages() if hasattr(site,\"getsitepackages\") else site.getusersitepackages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7834c74-00c5-4380-9401-abed800d4c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, time, numpy as np, torch\n",
    "from ultralytics import YOLO\n",
    "import mediapipe as mp\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f687bc50-9392-4063-9c30-b357982c4830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Simple IoU-based tracker ----------\n",
    "class Track:\n",
    "    def __init__(self, tid, bbox, conf):\n",
    "        self.id = tid\n",
    "        self.bbox = np.array(bbox, dtype=float)  # [x1,y1,x2,y2]\n",
    "        self.conf = float(conf)\n",
    "        self.age = 1\n",
    "        self.missed = 0\n",
    "\n",
    "class IoUTracker:\n",
    "    def __init__(self, iou_thresh=0.3, max_missed=3):\n",
    "        self.iou_thresh = iou_thresh\n",
    "        self.max_missed = max_missed\n",
    "        self.next_id = 1\n",
    "        self.tracks = []\n",
    "\n",
    "    @staticmethod\n",
    "    def iou(a, b):\n",
    "        # a,b: [x1,y1,x2,y2]\n",
    "        inter_x1 = max(a[0], b[0]); inter_y1 = max(a[1], b[1])\n",
    "        inter_x2 = min(a[2], b[2]); inter_y2 = min(a[3], b[3])\n",
    "        iw = max(0.0, inter_x2 - inter_x1); ih = max(0.0, inter_y2 - inter_y1)\n",
    "        inter = iw * ih\n",
    "        area_a = (a[2]-a[0])*(a[3]-a[1]); area_b = (b[2]-b[0])*(b[3]-b[1])\n",
    "        union = area_a + area_b - inter + 1e-6\n",
    "        return inter / union\n",
    "\n",
    "    def update(self, dets):\n",
    "        \"\"\"\n",
    "        dets: list of dicts -> {\"bbox\":[x1,y1,x2,y2], \"conf\":float}\n",
    "        returns: list of Track\n",
    "        \"\"\"\n",
    "        # 1) Build IoU matrix (tracks x detections)\n",
    "        T, D = len(self.tracks), len(dets)\n",
    "        iou_mat = np.zeros((T, D), dtype=float)\n",
    "        for i, tr in enumerate(self.tracks):\n",
    "            for j, d in enumerate(dets):\n",
    "                iou_mat[i, j] = self.iou(tr.bbox, d[\"bbox\"])\n",
    "\n",
    "        # 2) Greedy assignment by IoU\n",
    "        assigned_tr = set(); assigned_det = set()\n",
    "        pairs = []\n",
    "        while True:\n",
    "            if iou_mat.size == 0: break\n",
    "            i, j = divmod(iou_mat.argmax(), iou_mat.shape[1])\n",
    "            if iou_mat[i, j] < self.iou_thresh: break\n",
    "            pairs.append((i, j))\n",
    "            assigned_tr.add(i); assigned_det.add(j)\n",
    "            iou_mat[i, :] = -1; iou_mat[:, j] = -1\n",
    "\n",
    "        # 3) Update matched tracks\n",
    "        for i, j in pairs:\n",
    "            tr = self.tracks[i]\n",
    "            tr.bbox = np.array(dets[j][\"bbox\"], dtype=float)\n",
    "            tr.conf = float(dets[j][\"conf\"])\n",
    "            tr.age += 1\n",
    "            tr.missed = 0\n",
    "\n",
    "        # 4) Create new tracks for unmatched detections\n",
    "        for j, d in enumerate(dets):\n",
    "            if j in assigned_det: continue\n",
    "            self.tracks.append(Track(self.next_id, d[\"bbox\"], d[\"conf\"]))\n",
    "            self.next_id += 1\n",
    "\n",
    "        # 5) Age / remove unmatched tracks\n",
    "        alive = []\n",
    "        for idx, tr in enumerate(self.tracks):\n",
    "            if idx in assigned_tr:\n",
    "                alive.append(tr)\n",
    "            else:\n",
    "                tr.missed += 1\n",
    "                if tr.missed <= self.max_missed:\n",
    "                    alive.append(tr)\n",
    "        self.tracks = alive\n",
    "        return self.tracks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b9471d7-91c6-4af8-a5b2-3f999083c225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- YOLO detector wrapper ----------\n",
    "class YoloFaceDetector:\n",
    "    def __init__(self, weights=\"yolov12n-face.pt\", imgsz=640, conf=0.35, iou=0.5):\n",
    "        self.model = YOLO(weights)\n",
    "        self.imgsz = imgsz\n",
    "        self.conf = conf\n",
    "        self.iou = iou\n",
    "\n",
    "    def __call__(self, frame_bgr):\n",
    "        results = self.model.predict(\n",
    "            source=frame_bgr, imgsz=self.imgsz,\n",
    "            conf=self.conf, iou=self.iou, verbose=False, device=\"cpu\"\n",
    "        )[0]\n",
    "        dets = []\n",
    "        if results.boxes is not None and len(results.boxes) > 0:\n",
    "            xyxy = results.boxes.xyxy.cpu().numpy()\n",
    "            conf = results.boxes.conf.cpu().numpy()\n",
    "            for k in range(len(xyxy)):\n",
    "                x1, y1, x2, y2 = xyxy[k].tolist()\n",
    "                dets.append({\"bbox\": [x1, y1, x2, y2], \"conf\": float(conf[k])})\n",
    "        return dets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53fd9a0d-b549-45c5-9922-7e9f1b5c46d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1760424589.712677 354016555 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M3 Pro\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1760424589.716616 354019277 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1760424589.731808 354019276 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------ Utils ------------------------\n",
    "def clamp_box(x1, y1, x2, y2, w, h):\n",
    "    return [max(0, x1), max(0, y1), min(w-1, x2), min(h-1, y2)]\n",
    "\n",
    "def pad_and_square(b, pad, w, h):\n",
    "    x1,y1,x2,y2 = b\n",
    "    cx = (x1+x2)/2; cy = (y1+y2)/2; s = max(x2-x1, y2-y1) * (1+pad*2)\n",
    "    nx1 = cx - s/2; ny1 = cy - s/2; nx2 = cx + s/2; ny2 = cy + s/2\n",
    "    return clamp_box(nx1, ny1, nx2, ny2, w, h)\n",
    "\n",
    "def preprocess_face(frame_bgr, box, size=160):\n",
    "    x1,y1,x2,y2 = map(int, box); crop = frame_bgr[y1:y2, x1:x2]\n",
    "    crop = cv2.resize(crop, (size, size), interpolation=cv2.INTER_LINEAR)\n",
    "    rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "    # ImageNet-ish normalization (adapt to your training)\n",
    "    mean = np.array([0.485, 0.456, 0.406]); std = np.array([0.229, 0.224, 0.225])\n",
    "    rgb = (rgb - mean) / std\n",
    "    chw = np.transpose(rgb, (2,0,1))\n",
    "    return chw\n",
    "\n",
    "class EMA:\n",
    "    def __init__(self, alpha=0.6, init=None): self.a=alpha; self.v=init\n",
    "    def __call__(self, x):\n",
    "        self.v = x if self.v is None else self.a*x + (1-self.a)*self.v\n",
    "        return self.v\n",
    "\n",
    "# MediaPipe face mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=8,\n",
    "                             refine_landmarks=True, min_detection_confidence=0.5,\n",
    "                             min_tracking_confidence=0.5)\n",
    "\n",
    "def mouth_aspect_ratio(landmarks):\n",
    "    \"\"\"\n",
    "    Use MediaPipe FaceMesh indices:\n",
    "    mouth corners ~ 78 (left), 308 (right)\n",
    "    upper/lower inner lip center ~ 13 (upper), 14 (lower)\n",
    "    \"\"\"\n",
    "    p = landmarks\n",
    "    A = np.linalg.norm(p[13] - p[14])       # vertical\n",
    "    B = np.linalg.norm(p[78] - p[308])      # horizontal\n",
    "    return float(A/(B+1e-6))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19495d70-985a-477b-aec2-47aecc6314a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Main real-time loop ----------\n",
    "def main():\n",
    "    cap = cv2.VideoCapture(0)  # webcam\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH,  640)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "    win = \"YOLO + IOU+nonReID\"\n",
    "    cv2.namedWindow(win, cv2.WINDOW_NORMAL)\n",
    "    \n",
    "    # det = YoloFaceDetector(weights=\"face_yolo12_best.pt\", imgsz=640, conf=0.4)\n",
    "    # det = YoloFaceDetector(weights=\"best.pt\", imgsz=640, conf=0.4)\n",
    "    det = YoloFaceDetector(weights=\"yolov12n-face.pt\", imgsz=640, conf=0.4)\n",
    "    \n",
    "    # tracker = IoUTracker(iou_thresh=0.35, max_missed=3)\n",
    "    # ds_tracker = DeepSort(max_age=80, n_init=1, max_iou_distance=0.7, nn_budget=100)\n",
    "    ds_tracker = DeepSort(\n",
    "    max_age=80,\n",
    "    n_init=1,\n",
    "    max_iou_distance=0.7,\n",
    "    nn_budget=100,\n",
    "    embedder=None   # ðŸ‘ˆ disables CNN embeddings (IOU-only tracking)\n",
    ")\n",
    "\n",
    "\n",
    "    K = 3         # detect every K frames\n",
    "    fidx = 0\n",
    "    last_dets = []\n",
    "    try:\n",
    "        while True:\n",
    "            ok, frame = cap.read()\n",
    "            if not ok: break\n",
    "            fidx += 1\n",
    "    \n",
    "            # 1) DETECT every K-th frame; else reuse last detections (or keep empty and rely on tracker state)\n",
    "            if fidx % K == 1:\n",
    "                dets = det(frame)\n",
    "                last_dets = dets\n",
    "            else:\n",
    "                dets = last_dets  # optional: skip to save CPU\n",
    "    \n",
    "            # 2) TRACK with current detections\n",
    "            detections_xywh = []\n",
    "            for d in dets:\n",
    "                x1,y1,x2,y2 = d['bbox']\n",
    "                w, h = x2 - x1, y2 - y1\n",
    "                detections_xywh.append([[float(x1), float(y1), float(w), float(h)],\n",
    "                            float(d['conf']),\n",
    "                            \"face\"])  # label optional\n",
    "            # tracks = tracker.update(dets)\n",
    "    \n",
    "            # 3) Draw\n",
    "            tracks = ds_tracker.update_tracks(detections_xywh, frame=frame)  # pass frame to enable appearance\n",
    "            \n",
    "            for t in tracks:\n",
    "                if not t.is_confirmed():\n",
    "                    continue\n",
    "                x1,y1,x2,y2 = map(int, t.to_tlbr())          # left, top, right, bottom\n",
    "                tid = t.track_id\n",
    "                conf = getattr(t, \"det_conf\", 1.0)            # deep-sort-realtime exposes last det conf\n",
    "                cv2.rectangle(frame, (x1,y1), (x2,y2), (0,255,0), 2)\n",
    "                y_text = max(0, y1-6)\n",
    "                cv2.putText(frame, f\"ID {tid}\", (x1, y_text),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2)\n",
    "                txt = f\"Conf {conf:.2f}\"\n",
    "                (tw, _), _ = cv2.getTextSize(txt, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
    "                cv2.putText(frame, txt, (x2 - tw, y_text),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,255), 2)\n",
    "    \n",
    "            cv2.imshow(win, frame)\n",
    "\n",
    "            k = cv2.waitKey(1) & 0xFF\n",
    "            if k in (27, ord('q')): # ESC or q\n",
    "                print(dets)\n",
    "                break\n",
    "\n",
    "            # also exit if user clicks the window's X button\n",
    "            if cv2.getWindowProperty(win, cv2.WND_PROP_VISIBLE) < 1:\n",
    "                break\n",
    "    \n",
    "        # cap.release(); cv2.destroyAllWindows()\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyWindow(win)                # close just this window\n",
    "        # pump the event queue a few times so the OS actually closes it\n",
    "        for _ in range(3):\n",
    "            cv2.waitKey(1)\n",
    "        # tiny sleep can help on some macOS builds\n",
    "        time.sleep(0.05)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a40465e5-898f-439e-8917-d9ac4d559609",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Embedder not created during init so embeddings must be given now!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 49\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m     detections_xywh\u001b[38;5;241m.\u001b[39mappend([[\u001b[38;5;28mfloat\u001b[39m(x1), \u001b[38;5;28mfloat\u001b[39m(y1), \u001b[38;5;28mfloat\u001b[39m(w), \u001b[38;5;28mfloat\u001b[39m(h)],\n\u001b[1;32m     44\u001b[0m                 \u001b[38;5;28mfloat\u001b[39m(d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconf\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     45\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mface\u001b[39m\u001b[38;5;124m\"\u001b[39m])  \u001b[38;5;66;03m# label optional\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# tracks = tracker.update(dets)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# 3) Draw\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m tracks \u001b[38;5;241m=\u001b[39m \u001b[43mds_tracker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_tracks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdetections_xywh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pass frame to enable appearance\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tracks:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_confirmed():\n",
      "File \u001b[0;32m/opt/miniconda3/envs/face-detection/lib/python3.10/site-packages/deep_sort_realtime/deepsort_tracker.py:185\u001b[0m, in \u001b[0;36mDeepSort.update_tracks\u001b[0;34m(self, raw_detections, embeds, frame, today, others, instance_masks)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 185\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    186\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedder not created during init so embeddings must be given now!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    187\u001b[0m         )\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m frame \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meither embeddings or frame must be given!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Embedder not created during init so embeddings must be given now!"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b4288e-ddc3-4ff1-9765-f658ebd7aa95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (face-detection)",
   "language": "python",
   "name": "face-detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
