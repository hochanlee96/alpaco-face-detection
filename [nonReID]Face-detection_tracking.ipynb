{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7834c74-00c5-4380-9401-abed800d4c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, time, numpy as np, torch\n",
    "from ultralytics import YOLO\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f687bc50-9392-4063-9c30-b357982c4830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Simple IoU-based tracker ----------\n",
    "class Track:\n",
    "    def __init__(self, tid, bbox, conf):\n",
    "        self.id = tid\n",
    "        self.bbox = np.array(bbox, dtype=float)  # [x1,y1,x2,y2]\n",
    "        self.conf = float(conf)\n",
    "        self.age = 1\n",
    "        self.missed = 0\n",
    "\n",
    "class IoUTracker:\n",
    "    def __init__(self, iou_thresh=0.3, max_missed=3):\n",
    "        self.iou_thresh = iou_thresh\n",
    "        self.max_missed = max_missed\n",
    "        self.next_id = 1\n",
    "        self.tracks = []\n",
    "\n",
    "    @staticmethod\n",
    "    def iou(a, b):\n",
    "        # a,b: [x1,y1,x2,y2]\n",
    "        inter_x1 = max(a[0], b[0]); inter_y1 = max(a[1], b[1])\n",
    "        inter_x2 = min(a[2], b[2]); inter_y2 = min(a[3], b[3])\n",
    "        iw = max(0.0, inter_x2 - inter_x1); ih = max(0.0, inter_y2 - inter_y1)\n",
    "        inter = iw * ih\n",
    "        area_a = (a[2]-a[0])*(a[3]-a[1]); area_b = (b[2]-b[0])*(b[3]-b[1])\n",
    "        union = area_a + area_b - inter + 1e-6\n",
    "        return inter / union\n",
    "\n",
    "    def update(self, dets):\n",
    "        \"\"\"\n",
    "        dets: list of dicts -> {\"bbox\":[x1,y1,x2,y2], \"conf\":float}\n",
    "        returns: list of Track\n",
    "        \"\"\"\n",
    "        # 1) Build IoU matrix (tracks x detections)\n",
    "        T, D = len(self.tracks), len(dets)\n",
    "        iou_mat = np.zeros((T, D), dtype=float)\n",
    "        for i, tr in enumerate(self.tracks):\n",
    "            for j, d in enumerate(dets):\n",
    "                iou_mat[i, j] = self.iou(tr.bbox, d[\"bbox\"])\n",
    "\n",
    "        # 2) Greedy assignment by IoU\n",
    "        assigned_tr = set(); assigned_det = set()\n",
    "        pairs = []\n",
    "        while True:\n",
    "            if iou_mat.size == 0: break\n",
    "            i, j = divmod(iou_mat.argmax(), iou_mat.shape[1])\n",
    "            if iou_mat[i, j] < self.iou_thresh: break\n",
    "            pairs.append((i, j))\n",
    "            assigned_tr.add(i); assigned_det.add(j)\n",
    "            iou_mat[i, :] = -1; iou_mat[:, j] = -1\n",
    "\n",
    "        # 3) Update matched tracks\n",
    "        for i, j in pairs:\n",
    "            tr = self.tracks[i]\n",
    "            tr.bbox = np.array(dets[j][\"bbox\"], dtype=float)\n",
    "            tr.conf = float(dets[j][\"conf\"])\n",
    "            tr.age += 1\n",
    "            tr.missed = 0\n",
    "\n",
    "        # 4) Create new tracks for unmatched detections\n",
    "        for j, d in enumerate(dets):\n",
    "            if j in assigned_det: continue\n",
    "            self.tracks.append(Track(self.next_id, d[\"bbox\"], d[\"conf\"]))\n",
    "            self.next_id += 1\n",
    "\n",
    "        # 5) Age / remove unmatched tracks\n",
    "        alive = []\n",
    "        for idx, tr in enumerate(self.tracks):\n",
    "            if idx in assigned_tr:\n",
    "                alive.append(tr)\n",
    "            else:\n",
    "                tr.missed += 1\n",
    "                if tr.missed <= self.max_missed:\n",
    "                    alive.append(tr)\n",
    "        self.tracks = alive\n",
    "        return self.tracks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b9471d7-91c6-4af8-a5b2-3f999083c225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- YOLO detector wrapper ----------\n",
    "class YoloFaceDetector:\n",
    "    def __init__(self, weights=\"yolov12n-face.pt\", imgsz=640, conf=0.35, iou=0.5):\n",
    "        self.model = YOLO(weights)\n",
    "        self.imgsz = imgsz\n",
    "        self.conf = conf\n",
    "        self.iou = iou\n",
    "\n",
    "    def __call__(self, frame_bgr):\n",
    "        results = self.model.predict(\n",
    "            source=frame_bgr, imgsz=self.imgsz,\n",
    "            conf=self.conf, iou=self.iou, verbose=False, device=\"cpu\"\n",
    "        )[0]\n",
    "        dets = []\n",
    "        if results.boxes is not None and len(results.boxes) > 0:\n",
    "            xyxy = results.boxes.xyxy.cpu().numpy()\n",
    "            conf = results.boxes.conf.cpu().numpy()\n",
    "            for k in range(len(xyxy)):\n",
    "                x1, y1, x2, y2 = xyxy[k].tolist()\n",
    "                dets.append({\"bbox\": [x1, y1, x2, y2], \"conf\": float(conf[k])})\n",
    "        return dets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53fd9a0d-b549-45c5-9922-7e9f1b5c46d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1760403244.084181 353263407 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M3 Pro\n",
      "W0000 00:00:1760403244.087535 353297467 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1760403244.105107 353297469 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------ Utils ------------------------\n",
    "def clamp_box(x1, y1, x2, y2, w, h):\n",
    "    return [max(0, x1), max(0, y1), min(w-1, x2), min(h-1, y2)]\n",
    "\n",
    "def pad_and_square(b, pad, w, h):\n",
    "    x1,y1,x2,y2 = b\n",
    "    cx = (x1+x2)/2; cy = (y1+y2)/2; s = max(x2-x1, y2-y1) * (1+pad*2)\n",
    "    nx1 = cx - s/2; ny1 = cy - s/2; nx2 = cx + s/2; ny2 = cy + s/2\n",
    "    return clamp_box(nx1, ny1, nx2, ny2, w, h)\n",
    "\n",
    "def preprocess_face(frame_bgr, box, size=160):\n",
    "    x1,y1,x2,y2 = map(int, box); crop = frame_bgr[y1:y2, x1:x2]\n",
    "    crop = cv2.resize(crop, (size, size), interpolation=cv2.INTER_LINEAR)\n",
    "    rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "    # ImageNet-ish normalization (adapt to your training)\n",
    "    mean = np.array([0.485, 0.456, 0.406]); std = np.array([0.229, 0.224, 0.225])\n",
    "    rgb = (rgb - mean) / std\n",
    "    chw = np.transpose(rgb, (2,0,1))\n",
    "    return chw\n",
    "\n",
    "class EMA:\n",
    "    def __init__(self, alpha=0.6, init=None): self.a=alpha; self.v=init\n",
    "    def __call__(self, x):\n",
    "        self.v = x if self.v is None else self.a*x + (1-self.a)*self.v\n",
    "        return self.v\n",
    "\n",
    "# MediaPipe face mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=8,\n",
    "                             refine_landmarks=True, min_detection_confidence=0.5,\n",
    "                             min_tracking_confidence=0.5)\n",
    "\n",
    "def mouth_aspect_ratio(landmarks):\n",
    "    \"\"\"\n",
    "    Use MediaPipe FaceMesh indices:\n",
    "    mouth corners ~ 78 (left), 308 (right)\n",
    "    upper/lower inner lip center ~ 13 (upper), 14 (lower)\n",
    "    \"\"\"\n",
    "    p = landmarks\n",
    "    A = np.linalg.norm(p[13] - p[14])       # vertical\n",
    "    B = np.linalg.norm(p[78] - p[308])      # horizontal\n",
    "    return float(A/(B+1e-6))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "19495d70-985a-477b-aec2-47aecc6314a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Main real-time loop ----------\n",
    "def main():\n",
    "    cap = cv2.VideoCapture(0)  # webcam\n",
    "    win = \"YOLO + IOU+nonReID\"\n",
    "    cv2.namedWindow(win, cv2.WINDOW_NORMAL)\n",
    "    det = YoloFaceDetector(weights=\"best.pt\", imgsz=640, conf=0.4)\n",
    "    # det = YoloFaceDetector(weights=\"yolov12n-face.pt\", imgsz=640, conf=0.4)\n",
    "    tracker = IoUTracker(iou_thresh=0.35, max_missed=3)\n",
    "\n",
    "    K = 3         # detect every K frames\n",
    "    fidx = 0\n",
    "    last_dets = []\n",
    "    try:\n",
    "        while True:\n",
    "            ok, frame = cap.read()\n",
    "            if not ok: break\n",
    "            fidx += 1\n",
    "    \n",
    "            # 1) DETECT every K-th frame; else reuse last detections (or keep empty and rely on tracker state)\n",
    "            if fidx % K == 1:\n",
    "                dets = det(frame)\n",
    "                last_dets = dets\n",
    "            else:\n",
    "                dets = last_dets  # optional: skip to save CPU\n",
    "    \n",
    "            # 2) TRACK with current detections\n",
    "            tracks = tracker.update(dets)\n",
    "    \n",
    "            # 3) Draw\n",
    "            for tr in tracks:\n",
    "                x1, y1, x2, y2 = map(int, tr.bbox)\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0,255,0), 2)\n",
    "                cv2.putText(frame, f\"ID {tr.id}\", (x1, max(0,y1-6)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2)\n",
    "                text = f\"Conf {tr.conf:.2f}\"\n",
    "                text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]\n",
    "                text_x = x2 - text_size[0]   # align text to right edge\n",
    "                text_y = max(0, y1 - 6)\n",
    "                cv2.putText(frame, text, (text_x, text_y),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,255), 2)\n",
    "    \n",
    "            cv2.imshow(win, frame)\n",
    "\n",
    "            k = cv2.waitKey(1) & 0xFF\n",
    "            if k in (27, ord('q')): # ESC or q\n",
    "                print(dets)\n",
    "                break\n",
    "\n",
    "            # also exit if user clicks the window's X button\n",
    "            if cv2.getWindowProperty(win, cv2.WND_PROP_VISIBLE) < 1:\n",
    "                break\n",
    "    \n",
    "        # cap.release(); cv2.destroyAllWindows()\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyWindow(win)                # close just this window\n",
    "        # pump the event queue a few times so the OS actually closes it\n",
    "        for _ in range(3):\n",
    "            cv2.waitKey(1)\n",
    "        # tiny sleep can help on some macOS builds\n",
    "        time.sleep(0.05)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a40465e5-898f-439e-8917-d9ac4d559609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'bbox': [882.9578247070312, 492.9686279296875, 1239.11474609375, 938.6715087890625], 'conf': 0.8677162528038025}]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b4288e-ddc3-4ff1-9765-f658ebd7aa95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python object-detection",
   "language": "python",
   "name": "object-detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
