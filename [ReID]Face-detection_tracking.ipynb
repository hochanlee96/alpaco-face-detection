{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7834c74-00c5-4380-9401-abed800d4c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, time, numpy as np, torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from typing import List, Tuple\n",
    "import mediapipe as mp\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aee8609f-b336-4c54-a081-2bc798117732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- helpers ----------\n",
    "def iou_xyxy(a, b):\n",
    "    ax1, ay1, ax2, ay2 = a\n",
    "    bx1, by1, bx2, by2 = b\n",
    "    inter_x1, inter_y1 = max(ax1, bx1), max(ay1, by1)\n",
    "    inter_x2, inter_y2 = min(ax2, bx2), min(ay2, by2)\n",
    "    iw, ih = max(0, inter_x2 - inter_x1), max(0, inter_y2 - inter_y1)\n",
    "    inter = iw * ih\n",
    "    if inter <= 0: return 0.0\n",
    "    area_a = max(0, (ax2 - ax1)) * max(0, (ay2 - ay1))\n",
    "    area_b = max(0, (bx2 - bx1)) * max(0, (by2 - by1))\n",
    "    denom = area_a + area_b - inter\n",
    "    return 0.0 if denom <= 0 else inter / denom\n",
    "\n",
    "def normalize_dets_dict(dets_raw):\n",
    "    \"\"\"\n",
    "    Accepts detector output like:\n",
    "      [{'bbox':[x1,y1,x2,y2], 'conf':0.8}, ...]\n",
    "    Returns list of ((x1,y1,x2,y2), conf) with floats.\n",
    "    \"\"\"\n",
    "    clean = []\n",
    "    if not dets_raw:\n",
    "        return clean\n",
    "    for d in dets_raw:\n",
    "        try:\n",
    "            box = d.get('bbox', None)\n",
    "            conf = d.get('conf', None)\n",
    "            if box is None or conf is None or len(box) != 4:\n",
    "                continue\n",
    "            x1, y1, x2, y2 = map(float, box)\n",
    "            conf = float(conf)\n",
    "            clean.append(((x1, y1, x2, y2), conf))\n",
    "        except Exception as e:\n",
    "            # Skip any malformed entries\n",
    "            print(\"⚠️ Skipped invalid det:\", d, e)\n",
    "            continue\n",
    "    return clean\n",
    "\n",
    "def aggregate_temporal(dets_history, iou_merge_thr=0.6):\n",
    "    \"\"\"\n",
    "    dets_history: deque where each item is [((x1,y1,x2,y2), conf), ...]\n",
    "    Returns aggregated list [(bbox, conf), ...] using IOU clustering + confidence-weighted averaging.\n",
    "    \"\"\"\n",
    "    all_dets = []\n",
    "    for dets in dets_history:\n",
    "        if not dets: \n",
    "            continue\n",
    "        for (box, conf) in dets:\n",
    "            if box is None: \n",
    "                continue\n",
    "            x1,y1,x2,y2 = map(float, box)\n",
    "            all_dets.append([np.array([x1,y1,x2,y2], dtype=np.float32), float(conf)])\n",
    "\n",
    "    if not all_dets:\n",
    "        return []\n",
    "\n",
    "    clusters = []  # each: {'sum': sum(conf*box), 'w': sum(conf), 'max_conf': max(conf)}\n",
    "    for box, conf in all_dets:\n",
    "        matched = -1\n",
    "        best_iou = 0.0\n",
    "        # Compare with cluster average (sum/w)\n",
    "        for idx, c in enumerate(clusters):\n",
    "            avg_box = c['sum'] / max(c['w'], 1e-6)\n",
    "            i = iou_xyxy(box, avg_box)\n",
    "            if i > best_iou:\n",
    "                best_iou, matched = i, idx\n",
    "        if best_iou >= iou_merge_thr and matched >= 0:\n",
    "            clusters[matched]['sum'] += conf * box\n",
    "            clusters[matched]['w'] += conf\n",
    "            clusters[matched]['max_conf'] = max(clusters[matched]['max_conf'], conf)\n",
    "        else:\n",
    "            clusters.append({'sum': conf * box.copy(), 'w': conf, 'max_conf': conf})\n",
    "\n",
    "    aggregated = []\n",
    "    for c in clusters:\n",
    "        if c['w'] <= 0: \n",
    "            continue\n",
    "        avg_box = (c['sum'] / c['w']).astype(np.float32)\n",
    "        aggregated.append((tuple(avg_box.tolist()), float(c['max_conf'])))\n",
    "    return aggregated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f687bc50-9392-4063-9c30-b357982c4830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Utils\n",
    "# -----------------------------\n",
    "def iou_xyxy(a, b):\n",
    "    # a, b: (x1,y1,x2,y2)\n",
    "    ax1, ay1, ax2, ay2 = a\n",
    "    bx1, by1, bx2, by2 = b\n",
    "    inter_x1 = max(ax1, bx1); inter_y1 = max(ay1, by1)\n",
    "    inter_x2 = min(ax2, bx2); inter_y2 = min(ay2, by2)\n",
    "    iw = max(0, inter_x2 - inter_x1 + 1)\n",
    "    ih = max(0, inter_y2 - inter_y1 + 1)\n",
    "    inter = iw * ih\n",
    "    if inter == 0:\n",
    "        return 0.0\n",
    "    area_a = (ax2 - ax1 + 1) * (ay2 - ay1 + 1)\n",
    "    area_b = (bx2 - bx1 + 1) * (by2 - by1 + 1)\n",
    "    return inter / float(area_a + area_b - inter)\n",
    "\n",
    "def cosine_distance(a: np.ndarray, b: np.ndarray):\n",
    "    # a, b: (D,)\n",
    "    denom = (np.linalg.norm(a) * np.linalg.norm(b)) + 1e-12\n",
    "    return 1.0 - float(np.dot(a, b) / denom)  # 0 is identical, 2 is opposite\n",
    "\n",
    "def crop_xyxy(img, box):\n",
    "    x1,y1,x2,y2 = [int(v) for v in box]\n",
    "    h, w = img.shape[:2]\n",
    "    x1 = max(0, x1); y1 = max(0, y1)\n",
    "    x2 = min(w-1, x2); y2 = min(h-1, y2)\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return None\n",
    "    return img[y1:y2, x1:x2].copy()\n",
    "\n",
    "# -----------------------------\n",
    "# Appearance encoder (ReID embedder)\n",
    "# -----------------------------\n",
    "class ReIDEmbedder(nn.Module):\n",
    "    def __init__(self, device='cpu'):\n",
    "        super().__init__()\n",
    "        # Lightweight backbone: ResNet18 global pooled features (512-D)\n",
    "        # Tip: if weights fail to download in your environment, set weights=None and train/freeze later.\n",
    "        self.backbone = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        self.device = device\n",
    "        self.to(device).eval()\n",
    "\n",
    "        self.tf = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.Resize((128, 128)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225))\n",
    "        ])\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, imgs: List[np.ndarray]) -> np.ndarray:\n",
    "        if len(imgs) == 0:\n",
    "            return np.zeros((0, 512), dtype=np.float32)\n",
    "        batch = torch.stack([self.tf(im) for im in imgs]).to(self.device)\n",
    "        feats = self.backbone(batch)               # (N, 512)\n",
    "        feats = nn.functional.normalize(feats, p=2, dim=1)  # L2-normalize\n",
    "        return feats.detach().cpu().numpy()        # (N, 512)\n",
    "\n",
    "# -----------------------------\n",
    "# Track structure\n",
    "# -----------------------------\n",
    "class Track:\n",
    "    def __init__(self, tid: int, bbox, conf: float, emb: np.ndarray, frame_idx: int):\n",
    "        self.id = tid\n",
    "        self.bbox = np.array(bbox, dtype=np.float32)  # (x1,y1,x2,y2)\n",
    "        self.conf = float(conf)\n",
    "        self.emb = emb.astype(np.float32)  # (D,)\n",
    "        self.last_seen = frame_idx\n",
    "        self.age = 0       # frames since creation\n",
    "        self.time_since_update = 0\n",
    "        self.state = \"active\"  # \"active\" | \"lost\"\n",
    "\n",
    "    def update(self, bbox, conf, emb, frame_idx, emb_momentum=0.9):\n",
    "        self.bbox = np.array(bbox, dtype=np.float32)\n",
    "        self.conf = float(conf)\n",
    "        # EMA on embedding\n",
    "        self.emb = emb_momentum * self.emb + (1.0 - emb_momentum) * emb\n",
    "        self.emb = self.emb / (np.linalg.norm(self.emb) + 1e-12)\n",
    "        self.last_seen = frame_idx\n",
    "        self.time_since_update = 0\n",
    "        self.state = \"active\"\n",
    "        self.age += 1\n",
    "\n",
    "    def mark_lost(self):\n",
    "        self.state = \"lost\"\n",
    "\n",
    "# -----------------------------\n",
    "# IOU + ReID Tracker\n",
    "# -----------------------------\n",
    "class IOUReIDTracker:\n",
    "    def __init__(\n",
    "        self,\n",
    "        reid_encoder: ReIDEmbedder,\n",
    "        iou_thresh: float = 0.3,\n",
    "        reid_thresh: float = 0.35,     # cosine distance threshold (0 = identical)\n",
    "        max_age_lost: int = 50,        # keep lost tracks this many frames for re-id\n",
    "        w_iou: float = 0.5,            # cost blend: cost = w_iou*(1-IOU) + (1-w_iou)*cosine_dist\n",
    "        device: str = 'cpu'\n",
    "    ):\n",
    "        self.encoder = reid_encoder\n",
    "        self.iou_thresh = iou_thresh\n",
    "        self.reid_thresh = reid_thresh\n",
    "        self.max_age_lost = max_age_lost\n",
    "        self.w_iou = w_iou\n",
    "        self.device = device\n",
    "\n",
    "        self.tracks: List[Track] = []\n",
    "        self.next_id = 1\n",
    "        self.frame_idx = -1\n",
    "\n",
    "    def _match_by_iou(self, dets: List[Tuple], iou_thr: float):\n",
    "        # Greedy IOU matching\n",
    "        # dets: list of (bbox, conf, emb)\n",
    "        unmatched_tracks = list(range(len(self.tracks)))\n",
    "        unmatched_dets = list(range(len(dets)))\n",
    "        matches = []\n",
    "\n",
    "        if len(unmatched_tracks) == 0 or len(unmatched_dets) == 0:\n",
    "            return matches, unmatched_tracks, unmatched_dets\n",
    "\n",
    "        iou_matrix = np.zeros((len(self.tracks), len(dets)), dtype=np.float32)\n",
    "        for ti, tr in enumerate(self.tracks):\n",
    "            if tr.state != \"active\":\n",
    "                continue\n",
    "            for di, (dbox, _, _) in enumerate(dets):\n",
    "                iou_matrix[ti, di] = iou_xyxy(tr.bbox, dbox)\n",
    "\n",
    "        while True:\n",
    "            ti, di = np.unravel_index(np.argmax(iou_matrix), iou_matrix.shape)\n",
    "            max_iou = iou_matrix[ti, di]\n",
    "            if max_iou < iou_thr:\n",
    "                break\n",
    "            matches.append((ti, di))\n",
    "            # invalidate row/col\n",
    "            iou_matrix[ti, :] = -1\n",
    "            iou_matrix[:, di] = -1\n",
    "            if ti in unmatched_tracks: unmatched_tracks.remove(ti)\n",
    "            if di in unmatched_dets: unmatched_dets.remove(di)\n",
    "\n",
    "            if (iou_matrix > -1).sum() == 0:\n",
    "                break\n",
    "\n",
    "        return matches, unmatched_tracks, unmatched_dets\n",
    "\n",
    "    def _match_by_reid(self, lost_track_ids: List[int], dets: List[Tuple]):\n",
    "        # Lost tracks can be matched to *any* remaining det via appearance only\n",
    "        # cost = cosine distance; accept if below threshold\n",
    "        # We’ll do greedy min-cost.\n",
    "        if len(lost_track_ids) == 0 or len(dets) == 0:\n",
    "            return [], lost_track_ids, list(range(len(dets)))\n",
    "\n",
    "        cost = np.full((len(lost_track_ids), len(dets)), 1e3, dtype=np.float32)\n",
    "        for i, ti in enumerate(lost_track_ids):\n",
    "            tr = self.tracks[ti]\n",
    "            for j, (_, _, demb) in enumerate(dets):\n",
    "                cost[i, j] = cosine_distance(tr.emb, demb)\n",
    "\n",
    "        matches = []\n",
    "        used_tracks, used_dets = set(), set()\n",
    "        while True:\n",
    "            idx = np.unravel_index(np.argmin(cost), cost.shape)\n",
    "            i, j = idx\n",
    "            best = cost[i, j]\n",
    "            if best > self.reid_thresh:\n",
    "                break\n",
    "            ti = lost_track_ids[i]\n",
    "            if ti in used_tracks or j in used_dets:\n",
    "                cost[i, j] = 1e3\n",
    "                continue\n",
    "            matches.append((ti, j))\n",
    "            used_tracks.add(ti); used_dets.add(j)\n",
    "            cost[i, :] = 1e3\n",
    "            cost[:, j] = 1e3\n",
    "            if (cost < 1e3).sum() == 0:\n",
    "                break\n",
    "\n",
    "        unmatched_tracks = [ti for ti in lost_track_ids if ti not in used_tracks]\n",
    "        unmatched_dets = [j for j in range(len(dets)) if j not in used_dets]\n",
    "        return matches, unmatched_tracks, unmatched_dets\n",
    "\n",
    "    def update(self, frame_bgr: np.ndarray, detections: List[Tuple[Tuple[int,int,int,int], float]]):\n",
    "        \"\"\"\n",
    "        detections: list of (bbox, conf) with bbox=(x1,y1,x2,y2)\n",
    "        Returns: list of active tracks\n",
    "        \"\"\"\n",
    "        self.frame_idx += 1\n",
    "\n",
    "        # STEP 0: extract embeddings for all detections\n",
    "        det_crops, valid_idx = [], []\n",
    "        for i, (box, _) in enumerate(detections):\n",
    "            crop = crop_xyxy(frame_bgr, box)\n",
    "            if crop is None or crop.size == 0:\n",
    "                continue\n",
    "            det_crops.append(crop); valid_idx.append(i)\n",
    "\n",
    "        det_embs = self.encoder(det_crops)  # (M, 512)\n",
    "        # Rebuild dets with embeddings (skip invalid crops)\n",
    "        dets = []\n",
    "        for k, vi in enumerate(valid_idx):\n",
    "            box, conf = detections[vi]\n",
    "            dets.append((np.array(box, dtype=np.float32), float(conf), det_embs[k]))\n",
    "\n",
    "        # Mark aging & move old actives to lost if not updated for long\n",
    "        for tr in self.tracks:\n",
    "            tr.time_since_update += 1\n",
    "            # Passive aging: we won't drop immediately; we handle pruning below.\n",
    "\n",
    "        # STEP 1: IOU matching among ACTIVE tracks\n",
    "        active_ids = [i for i, tr in enumerate(self.tracks) if tr.state == \"active\"]\n",
    "        matches_iou, unmatched_tracks, unmatched_dets = self._match_by_iou(dets, self.iou_thresh)\n",
    "\n",
    "        # Apply IOU matches\n",
    "        used_det_ids = set()\n",
    "        for ti, di in matches_iou:\n",
    "            tr = self.tracks[ti]\n",
    "            dbox, dconf, demb = dets[di]\n",
    "            tr.update(dbox, dconf, demb, self.frame_idx)\n",
    "            used_det_ids.add(di)\n",
    "\n",
    "        # STEP 2: ReID matching — allow LOST tracks to come back\n",
    "        # Collect candidate lost tracks that are within reid horizon\n",
    "        lost_candidates = []\n",
    "        for i, tr in enumerate(self.tracks):\n",
    "            if tr.state == \"lost\":\n",
    "                if (self.frame_idx - tr.last_seen) <= self.max_age_lost:\n",
    "                    lost_candidates.append(i)\n",
    "\n",
    "        # Build list of remaining dets (not matched yet)\n",
    "        remaining_dets = [dets[i] for i in range(len(dets)) if i not in used_det_ids]\n",
    "        reid_matches, unmatched_lost, remaining_unmatched_det_indices = self._match_by_reid(lost_candidates, remaining_dets)\n",
    "\n",
    "        # Map remaining_dets indices back to original det indices\n",
    "        rem_map = [i for i in range(len(dets)) if i not in used_det_ids]\n",
    "        for ti, j in reid_matches:\n",
    "            di = rem_map[j]\n",
    "            dbox, dconf, demb = dets[di]\n",
    "            tr = self.tracks[ti]\n",
    "            tr.update(dbox, dconf, demb, self.frame_idx)\n",
    "            used_det_ids.add(di)\n",
    "\n",
    "        # STEP 3: Create new tracks for unmatched detections\n",
    "        for di in range(len(dets)):\n",
    "            if di in used_det_ids:\n",
    "                continue\n",
    "            dbox, dconf, demb = dets[di]\n",
    "            new_tr = Track(self.next_id, dbox, dconf, demb, self.frame_idx)\n",
    "            self.tracks.append(new_tr)\n",
    "            self.next_id += 1\n",
    "\n",
    "        # STEP 4: Mark unmatched ACTIVE tracks as LOST\n",
    "        unmatched_active = [i for i in unmatched_tracks if self.tracks[i].state == \"active\"]\n",
    "        for ti in unmatched_active:\n",
    "            self.tracks[ti].mark_lost()\n",
    "\n",
    "        # STEP 5: Prune very old LOST tracks\n",
    "        pruned = []\n",
    "        for tr in self.tracks:\n",
    "            if tr.state == \"lost\" and (self.frame_idx - tr.last_seen) > self.max_age_lost:\n",
    "                pruned.append(tr.id)\n",
    "        if pruned:\n",
    "            self.tracks = [tr for tr in self.tracks if not (tr.state == \"lost\" and (self.frame_idx - tr.last_seen) > self.max_age_lost)]\n",
    "\n",
    "        # Return ACTIVE tracks only\n",
    "        return [tr for tr in self.tracks if tr.state == \"active\"]\n",
    "\n",
    "# -----------------------------\n",
    "# Example usage per frame\n",
    "# -----------------------------\n",
    "# Initialize once\n",
    "\n",
    "\n",
    "# In your video loop:\n",
    "# detections = [(bbox, conf), ...]  # from your detector\n",
    "# tracks = tracker.update(frame, detections)\n",
    "# for tr in tracks:\n",
    "#     x1,y1,x2,y2 = map(int, tr.bbox)\n",
    "#     cv2.rectangle(frame, (x1,y1), (x2,y2), (0,255,0), 2)\n",
    "#     # top-left: ID\n",
    "#     cv2.putText(frame, f\"ID {tr.id}\", (x1, max(0, y1-6)),\n",
    "#                 cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2)\n",
    "#     # top-right: Conf\n",
    "#     txt = f\"Conf {tr.conf:.2f}\"\n",
    "#     (tw, th), _ = cv2.getTextSize(txt, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
    "#     cv2.putText(frame, txt, (x2 - tw, max(0, y1-6)),\n",
    "#                 cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,255), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b9471d7-91c6-4af8-a5b2-3f999083c225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- YOLO detector wrapper ----------\n",
    "class YoloFaceDetector:\n",
    "    def __init__(self, weights=\"yolov12n-face.pt\", imgsz=640, conf=0.35, iou=0.5):\n",
    "        self.model = YOLO(weights)\n",
    "        self.imgsz = imgsz\n",
    "        self.conf = conf\n",
    "        self.iou = iou\n",
    "\n",
    "    def __call__(self, frame_bgr):\n",
    "        results = self.model.predict(\n",
    "            source=frame_bgr, imgsz=self.imgsz,\n",
    "            conf=self.conf, iou=self.iou, verbose=False, device=\"cpu\"\n",
    "        )[0]\n",
    "        dets = []\n",
    "        if results.boxes is not None and len(results.boxes) > 0:\n",
    "            xyxy = results.boxes.xyxy.cpu().numpy()\n",
    "            conf = results.boxes.conf.cpu().numpy()\n",
    "            for k in range(len(xyxy)):\n",
    "                x1, y1, x2, y2 = xyxy[k].tolist()\n",
    "                dets.append({\"bbox\": [x1, y1, x2, y2], \"conf\": float(conf[k])})\n",
    "        return dets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53fd9a0d-b549-45c5-9922-7e9f1b5c46d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1760405981.895969 353355743 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M3 Pro\n",
      "W0000 00:00:1760405981.899737 353372820 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1760405981.923389 353372819 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------ Utils ------------------------\n",
    "def clamp_box(x1, y1, x2, y2, w, h):\n",
    "    return [max(0, x1), max(0, y1), min(w-1, x2), min(h-1, y2)]\n",
    "\n",
    "def pad_and_square(b, pad, w, h):\n",
    "    x1,y1,x2,y2 = b\n",
    "    cx = (x1+x2)/2; cy = (y1+y2)/2; s = max(x2-x1, y2-y1) * (1+pad*2)\n",
    "    nx1 = cx - s/2; ny1 = cy - s/2; nx2 = cx + s/2; ny2 = cy + s/2\n",
    "    return clamp_box(nx1, ny1, nx2, ny2, w, h)\n",
    "\n",
    "def preprocess_face(frame_bgr, box, size=160):\n",
    "    x1,y1,x2,y2 = map(int, box); crop = frame_bgr[y1:y2, x1:x2]\n",
    "    crop = cv2.resize(crop, (size, size), interpolation=cv2.INTER_LINEAR)\n",
    "    rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "    # ImageNet-ish normalization (adapt to your training)\n",
    "    mean = np.array([0.485, 0.456, 0.406]); std = np.array([0.229, 0.224, 0.225])\n",
    "    rgb = (rgb - mean) / std\n",
    "    chw = np.transpose(rgb, (2,0,1))\n",
    "    return chw\n",
    "\n",
    "class EMA:\n",
    "    def __init__(self, alpha=0.6, init=None): self.a=alpha; self.v=init\n",
    "    def __call__(self, x):\n",
    "        self.v = x if self.v is None else self.a*x + (1-self.a)*self.v\n",
    "        return self.v\n",
    "\n",
    "# MediaPipe face mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=8,\n",
    "                             refine_landmarks=True, min_detection_confidence=0.5,\n",
    "                             min_tracking_confidence=0.5)\n",
    "\n",
    "def mouth_aspect_ratio(landmarks):\n",
    "    \"\"\"\n",
    "    Use MediaPipe FaceMesh indices:\n",
    "    mouth corners ~ 78 (left), 308 (right)\n",
    "    upper/lower inner lip center ~ 13 (upper), 14 (lower)\n",
    "    \"\"\"\n",
    "    p = landmarks\n",
    "    A = np.linalg.norm(p[13] - p[14])       # vertical\n",
    "    B = np.linalg.norm(p[78] - p[308])      # horizontal\n",
    "    return float(A/(B+1e-6))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8278c7fb-8bb1-48bf-ae81-8fbadd61c243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- helpers ----------\n",
    "def iou_xyxy(a, b):\n",
    "    ax1, ay1, ax2, ay2 = a\n",
    "    bx1, by1, bx2, by2 = b\n",
    "    inter_x1, inter_y1 = max(ax1, bx1), max(ay1, by1)\n",
    "    inter_x2, inter_y2 = min(ax2, bx2), min(ay2, by2)\n",
    "    iw, ih = max(0, inter_x2 - inter_x1), max(0, inter_y2 - inter_y1)\n",
    "    inter = iw * ih\n",
    "    if inter <= 0: return 0.0\n",
    "    area_a = max(0, (ax2 - ax1)) * max(0, (ay2 - ay1))\n",
    "    area_b = max(0, (bx2 - bx1)) * max(0, (by2 - by1))\n",
    "    denom = area_a + area_b - inter\n",
    "    return 0.0 if denom <= 0 else inter / denom\n",
    "\n",
    "def aggregate_temporal(dets_history, iou_merge_thr=0.6):\n",
    "    \"\"\"\n",
    "    dets_history: deque of lists; each list is [(bbox, conf), ...] with bbox=(x1,y1,x2,y2)\n",
    "    Returns aggregated list [(bbox, conf), ...] using IOU-based clustering and confidence-weighted averaging.\n",
    "    \"\"\"\n",
    "    all_dets = []\n",
    "    for dets in dets_history:\n",
    "        if not dets: \n",
    "            continue\n",
    "        for (box, conf) in dets:\n",
    "            if box is None: \n",
    "                continue\n",
    "            x1,y1,x2,y2 = map(float, box)\n",
    "            all_dets.append([np.array([x1,y1,x2,y2], dtype=np.float32), float(conf)])\n",
    "\n",
    "    if not all_dets:\n",
    "        return []\n",
    "\n",
    "    clusters = []  # each: dict(keys: 'sum', 'w', 'max_conf')\n",
    "    for box, conf in all_dets:\n",
    "        matched = -1\n",
    "        best_iou = 0.0\n",
    "        for idx, c in enumerate(clusters):\n",
    "            i = iou_xyxy(box, c['sum'] / max(c['w'], 1e-6))\n",
    "            if i > best_iou:\n",
    "                best_iou, matched = i, idx\n",
    "        if best_iou >= iou_merge_thr and matched >= 0:\n",
    "            clusters[matched]['sum'] += conf * box\n",
    "            clusters[matched]['w'] += conf\n",
    "            clusters[matched]['max_conf'] = max(clusters[matched]['max_conf'], conf)\n",
    "        else:\n",
    "            clusters.append({'sum': conf * box.copy(), 'w': conf, 'max_conf': conf})\n",
    "\n",
    "    aggregated = []\n",
    "    for c in clusters:\n",
    "        if c['w'] <= 0: \n",
    "            continue\n",
    "        avg_box = (c['sum'] / c['w']).astype(np.float32)\n",
    "        aggregated.append((tuple(avg_box.tolist()), float(c['max_conf'])))\n",
    "    return aggregated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "19495d70-985a-477b-aec2-47aecc6314a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    cap = cv2.VideoCapture(0)  # webcam\n",
    "    win = \"YOLO + IOU+ReID (Temporal-agg)\"\n",
    "    cv2.namedWindow(win, cv2.WINDOW_NORMAL)\n",
    "    # det = YoloFaceDetector(weights=\"best.pt\", imgsz=640, conf=0.4)\n",
    "    det = YoloFaceDetector(weights=\"yolov12n-face.pt\", imgsz=640, conf=0.4)\n",
    "    # det = YoloFaceDetector(weights=\"face_yolo12_best.pt\", imgsz=640, conf=0.4)\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    embedder = ReIDEmbedder(device=device)\n",
    "    tracker = IOUReIDTracker(embedder, iou_thresh=0.4, reid_thresh=0.35, max_age_lost=80, w_iou=0.5, device=device)\n",
    "\n",
    "    K = 3                      # run detector every K frames\n",
    "    fidx = 0\n",
    "    last_dets = []             # normalized detections\n",
    "    A = 3                      # aggregate the last A detection frames\n",
    "    det_history = deque(maxlen=A)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            ok, frame = cap.read()\n",
    "            if not ok:\n",
    "                break\n",
    "            fidx += 1\n",
    "    \n",
    "            # 1) DETECT every Kth frame; otherwise reuse last normalized dets\n",
    "            if fidx % K == 1:\n",
    "                raw = det(frame) or []        # -> [{'bbox': [...], 'conf': ...}, ...]\n",
    "                dets = normalize_dets_dict(raw)\n",
    "                last_dets = dets\n",
    "            else:\n",
    "                dets = last_dets\n",
    "    \n",
    "            # 2) aggregate recent detections\n",
    "            det_history.append(dets)\n",
    "            agg_dets = aggregate_temporal(det_history, iou_merge_thr=0.6)  # -> [((x1,y1,x2,y2), conf), ...]\n",
    "    \n",
    "            # 3) TRACK\n",
    "            tracks = tracker.update(frame, agg_dets)\n",
    "    \n",
    "            # 4) Draw: ID (top-left), Conf (top-right)\n",
    "            for tr in tracks:\n",
    "                x1, y1, x2, y2 = map(int, tr.bbox)\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0,255,0), 2)\n",
    "                \n",
    "    \n",
    "                y_text = max(0, y1 - 6)\n",
    "                # ID left\n",
    "                cv2.putText(frame, f\"ID {tr.id}\", (x1, y_text),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2)\n",
    "                # Conf right\n",
    "                text = f\"Conf {tr.conf:.2f}\"\n",
    "                (tw, th), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
    "                cv2.putText(frame, text, (x2 - tw, y_text),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,255), 2)\n",
    "    \n",
    "            cv2.imshow(win, frame)\n",
    "            k = cv2.waitKey(1) & 0xFF\n",
    "            if k in (27, ord('q')): # ESC or q\n",
    "                print(dets)\n",
    "                break\n",
    "\n",
    "            # also exit if user clicks the window's X button\n",
    "            if cv2.getWindowProperty(win, cv2.WND_PROP_VISIBLE) < 1:\n",
    "                break\n",
    "    \n",
    "        # cap.release()\n",
    "        # cv2.destroyAllWindows()\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyWindow(win)                # close just this window\n",
    "        # pump the event queue a few times so the OS actually closes it\n",
    "        for _ in range(3):\n",
    "            cv2.waitKey(1)\n",
    "        # tiny sleep can help on some macOS builds\n",
    "        time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a40465e5-898f-439e-8917-d9ac4d559609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((881.050048828125, 513.7398071289062, 1239.62890625, 932.495361328125), 0.8655760288238525), ((0.0, 610.517578125, 123.80026245117188, 984.6851806640625), 0.7591698169708252), ((392.1365661621094, 822.3052368164062, 461.5881042480469, 900.2393188476562), 0.7365306615829468)]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b4288e-ddc3-4ff1-9765-f658ebd7aa95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python object-detection",
   "language": "python",
   "name": "object-detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
